# The spotify_df file contains more than 170.000 songs collected from Spotify Web API, and also you can find spotify_df grouped by artist, year, or genre in the spotify_df section.
#
# Primary:
#
# id (Id of track generated by Spotify)
# Numerical:
#
# acousticness (Ranges from 0 to 1)
# danceability (Ranges from 0 to 1)
# energy (Ranges from 0 to 1)
# duration_ms (Integer typically ranging from 200k to 300k)
# instrumentalness (Ranges from 0 to 1)
# valence (Ranges from 0 to 1)- happiness index
# popularity (Ranges from 0 to 100)- based on the number of times the song was played on spotify.
# tempo (Float typically ranging from 50 to 150)
# liveness (Ranges from 0 to 1)-indicates whether the song was played live or not
# loudness (Float typically ranging from -60 to 0)
# speechiness (Ranges from 0 to 1)
# year (Ranges from 1921 to 2020)
# Dummy:
#
# mode (0 = Minor, 1 = Major)
# explicit (0 = No explicit content, 1 = Explicit content)
# Categorical:
#
# key (All keys on octave encoded as values ranging from 0 to 11, starting on C as 0, C# as 1 and so onâ€¦)
# artists (List of artists mentioned)
# release_date (Date of release mostly in yyyy-mm-dd format, however precision of date may vary)
# name (Name of the song)
# spotify_dfset Limitations:
# There are only nearly 2000 songs from each year. The popularity metric(range 0-100) is based on the number of times the song was played on spotify,
# Thus, it is naturally lower for songs in the older decades and higher for songs in the present decade
import time
import warnings

import ipywidgets as widgets
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import seaborn as sns
from IPython.display import display
from sklearn import decomposition
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler  # for normalizing spotify_df

warnings.filterwarnings("ignore")

print("Import successful")

# Our objective her would be to clean the spotify_df, perform feature engineering, Explore the spotify_df and then get valuable insights from it. After this,
# we try to cluster the songs according to their numerical attributes so that we can Categorize them into different categories, for eg. Live songs,
# instrumental songs, danceable songs etc. This is esentially the genre prediction of songs. We perform dimensionality reduction for clustering.
# I have Used Principal Component Analysis(PCA) for the same

spotify_df = pd.read_csv("C:\\Users\\Admin01\\PycharmProjects\\DEMO1\\Project2\\data.csv")
time.sleep(5)
print(spotify_df.tail())

print(spotify_df.shape)
print(spotify_df.isna().sum())
print(spotify_df[spotify_df['name'] == 'Adore You'])
# We can see that these duplicatre values might arise due to release of Album versions, etc. The ID is different for both the songs.
# We remove such duplicate values.
spotify_df.drop_duplicates(subset=['artists', 'name'], inplace=True)
print(spotify_df.shape)
print(spotify_df.describe())

# Feature Engineering
# Creating a categorical decade column for the purpose of EDA

year_list = sorted(list(spotify_df['year'].unique()))
res = pd.cut(spotify_df['year'], 10,
             labels=['1921-31', '1931-41', '1941-51', '1951-61', '1961-71', '1971-81', '1981-91', '1991-01', '2001-11',
                     '2011-2020'])
spotify_df['decade'] = res
print(spotify_df.head(2))
spotify_df.to_csv('clean_spotify_df_spotify.csv', index=False)

spotify_df = pd.read_csv("C:\\Users\\Admin01\\PycharmProjects\\DEMO1\\Project2\\clean_spotify_df_spotify.csv")

print(spotify_df['decade'].value_counts())
count = dict(spotify_df['decade'].value_counts())
fig = px.pie(values=list(count.values()), names=list(count.keys()))
fig.show()

# We can see that we have a fair distribution of spotify_df from alll decades except for the decades 1921-31 and 1931-41.
# we have less songs from theses decades in the spotify_df
#
# Mode: Major(1) / Minor(0)

print(spotify_df['mode'].value_counts())
count = dict(spotify_df['mode'].value_counts())
fig = px.pie(values=list(count.values()), names=list(count.keys()))
fig.show()

# Most of the songs have mode : major
# Explicit content(1) or 'no explicit content'(0)

print(spotify_df['explicit'].value_counts())
count = dict(spotify_df['explicit'].value_counts())
fig = px.pie(values=list(count.values()), names=list(count.keys()))
fig.show()

# Widgets(A dropdown) for popular songs of every year
# Using these widgets, it is possible to choose a year from the list of years 1920-2020 and find the most popular song of that year
all = 'ALL'


def unique_sorted_values(array):
    distinct = array.unique().tolist()
    distinct = sorted(distinct)
    distinct.insert(0, all)
    return distinct


year = widgets.Dropdown(options=unique_sorted_values(spotify_df['year']))


def condition(change):
    if (change.new == all):
        df = spotify_df.sort_values('popularity', ascending=False)[['name', 'popularity']][0:5]
        print(df)
        plt.figure(figsize=(10, 3))
        plt.barh(df['name'], df['popularity'], color=('cyan', 'blue'))
        plt.show()
    else:
        df = spotify_df[spotify_df['year'] == change.new]
        df = df.sort_values('popularity', ascending=False)[['name', 'popularity']][0:5]
        print(df)
        plt.figure(figsize=(10, 3))
        plt.barh(df['name'], df['popularity'], color=('maroon', 'olive'))
        plt.show()


year.observe(condition, names='value')
display(year)

lst = ['acousticness', 'danceability', 'valence', 'loudness', 'energy', 'duration_ms']

factors = widgets.Dropdown(options=lst)


def condition(change):
    for i in range(len(lst)):
        if change.new == lst[i]:
            df = spotify_df.sort_values(lst[i], ascending=False)[['name', lst[i]]][0:5]
            print(df)
            plt.figure(figsize=(10, 3))
            plt.barh(df['name'], df[lst[i]], color=('maroon', 'olive'))
            plt.show()


factors.observe(condition, names='value')
display(factors)

# Above, we can see these are the top 5 longest songs in the spotify_dfset
# Popularity throughout the decades : is popularity affected by type of content(explicit/implicit) ?
plt.figure(figsize=(12, 7))
sns.boxplot(x='decade', y='popularity', hue='explicit', data=spotify_df)
plt.title('year wise popularity and explicit content')
plt.show()

# There was very less explicit content before 1960s. Songs mostly started having explicit contents since 1970s.
# Songs having explicit content were in general, less popular than those that did not have explicit content.
# In the recent decade, we can see that songs having explicit content are more popular than those that
# do not have explicit content. This also shows the change of culture through the decade.
# Also, it is important to note that popularity increased over years because in this spotify_dfset, it is a metric which is
# measured on the basis of how many times the song is played. Due to this, older songs have less populariy.
print(spotify_df.head(5))
# To check if popularity is affected by mode
sns.barplot(x='key', y='popularity', hue='mode', data=spotify_df, ci=None)
plt.legend(loc='best', bbox_to_anchor=(0.8, 0., 0.5, 0.9))
plt.show()

# A combination of mode 0 and key 1 is more likely to get ur song popular
# spotify_dfframe for clustering
cluster = spotify_df[
    ['valence', 'acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness', 'loudness',
     'speechiness', 'tempo']]
cluster.head(5)
minmax = MinMaxScaler()
cluster['duration_ms'] = minmax.fit_transform(np.array(cluster['duration_ms']).reshape(-1, 1))
cluster['loudness'] = minmax.fit_transform(np.array(cluster['loudness']).reshape(-1, 1))
cluster['tempo'] = minmax.fit_transform(np.array(cluster['tempo']).reshape(-1, 1))
cluster.head()
pca = decomposition.PCA()
pca.n_components = 2
pca_df = pca.fit_transform(cluster)
print(pca_df)

wcss_list = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(cluster)
    wcss_list.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss_list)
plt.title('elbow method')
plt.xlabel('No of clusters(k)')
plt.ylabel('wcss')
plt.show()
# There is a decrease of wcss at k = 3 . I Choose k as 3.
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
y_pred = kmeans.fit_predict(cluster)
print(y_pred)

plt.figure(figsize=(15, 10))
plt.scatter(pca_df[y_pred == 0, 0], pca_df[y_pred == 0, 1], s=100, c='blue', label='Cluster 1')
plt.scatter(pca_df[y_pred == 1, 0], pca_df[y_pred == 1, 1], s=100, c='green', label='Cluster 2')
plt.scatter(pca_df[y_pred == 2, 0], pca_df[y_pred == 2, 1], s=100, c='red', label='Cluster 3')
plt.scatter(pca_df[y_pred == 3, 0], pca_df[y_pred == 3, 1], s=100, c='maroon', label='Cluster 4')
plt.scatter(pca_df[y_pred == 4, 0], pca_df[y_pred == 4, 1], s=100, c='cyan', label='Cluster 5')

plt.title('Clusters of songs')
plt.xlabel('Principal_Comp1')
plt.ylabel('Principal_Comp2')
plt.show()

cluster['label'] = y_pred
print(cluster.head())

silhouette_score(cluster, cluster['label'], metric='euclidean')
# The silhouette score is greater than 0.5. Its away from 0 which indicates that the clustering we have performed is
# fairly good and there are not significant overlaps between clusters.
table = pd.pivot_table(cluster, index=['label'], aggfunc=np.mean)
print(table)

# Cluster Analysis
# Cluster 0: These songs are less acoustic/instrumental. This cluster consists of song which have highest loudness, energy
# and tempo. This is definitely the most dynamic cluster.They have the highest valence. These songs are the songs with happiest vibes.
# Cluster 1: This cluster has songs which are most likely to have been played live, since the liveness and speechiness
# index are the highest. This cluster also has high acousticnesss.
# Cluster 2: These songs have the highest acousticness and instrumentalness. This cluster has the slowest music style.

# Heirarchichal CLustering
# I have taken a random sample of 10000 points from our dataset . Then, I have performed heirarchichal clustering
cluster = cluster.drop(columns=['label'])
cluster_sample = cluster.sample(n=10000)
cluster_sample.head()
print(cluster_sample.shape)
pca = decomposition.PCA()
pca.n_components = 2
pca_df = pca.fit_transform(cluster_sample)
print('Heirarchichal Clustering\n')
print(pca_df)
print(pca_df.shape)
# Choosing k=3 for Heirarchichal CLustering

hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
y_pred = hc.fit_predict(cluster_sample)

plt.figure(figsize=(15, 10))
plt.scatter(pca_df[y_pred == 0, 0], pca_df[y_pred == 0, 1], s=100, c='blue', label='Cluster 1')
plt.scatter(pca_df[y_pred == 1, 0], pca_df[y_pred == 1, 1], s=100, c='green', label='Cluster 2')
plt.scatter(pca_df[y_pred == 2, 0], pca_df[y_pred == 2, 1], s=100, c='red', label='Cluster 3')
plt.scatter(pca_df[y_pred == 3, 0], pca_df[y_pred == 3, 1], s=100, c='maroon', label='Cluster 4')
plt.scatter(pca_df[y_pred == 4, 0], pca_df[y_pred == 4, 1], s=100, c='cyan', label='Cluster 5')

plt.title('Clusters of songs')
plt.xlabel('Principal_Comp1')
plt.ylabel('Principal_Comp2')
plt.show()

cluster_sample['Label'] = y_pred
silhouette_score(cluster_sample, cluster_sample['Label'], metric='euclidean')

# The silhouette score is greater than 0.5. Its away from 0 which indicates that the clustering we have performed is
# fairly good and there are not significant overlaps between clusters.

table = pd.pivot_table(cluster_sample, index=['Label'], aggfunc=np.mean)
print(table)
